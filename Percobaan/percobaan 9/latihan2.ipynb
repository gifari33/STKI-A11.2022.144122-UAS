{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hisya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hisya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah total data setelah menghapus duplikat: 98862\n",
      "Jumlah positif: 98862\n",
      "Jumlah negatif: 0\n",
      "Data yang sudah diproses disimpan di: H:\\My Drive\\analisa sentimen git\\percobaan 9\\validation_results_processed.csv\n",
      "                                             content  \\\n",
      "0                            akun gopay saya di blok   \n",
      "1  lambat sekali sekarang ini bosssku apk gojek g...   \n",
      "2  baru download gojek dan hape baru trus ditop u...   \n",
      "3                                          coba dulu   \n",
      "4  gimana ini kak pin saya salah terus padahal ud...   \n",
      "\n",
      "                                   processed_content  \n",
      "0                                    akun gopay blok  \n",
      "1                   lambat bosssku apk gojek gk kaya  \n",
      "2  download gojek hape trus top u gopay transaksi...  \n",
      "3                                               coba  \n",
      "4               gimana kak pin salah udah ubah salah  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "# Pastikan stopwords dan punkt sudah diunduh\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Membuat stemmer untuk bahasa Indonesia menggunakan Sastrawi\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# Fungsi untuk normalisasi teks\n",
    "def normalize_text(text):\n",
    "    # Menghapus karakter non-alfanumerik dan mengubah menjadi huruf kecil\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return text.lower()\n",
    "\n",
    "# Fungsi untuk tokenisasi, stop word removal, dan stemming\n",
    "def preprocess_text(text):\n",
    "    # Normalisasi\n",
    "    text = normalize_text(text)\n",
    "    \n",
    "    # Tokenisasi\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Stop word removal\n",
    "    stop_words = set(stopwords.words('indonesian'))  # Pastikan 'indonesian' telah didownload\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "    \n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "# Membaca file CSV\n",
    "file_path = r'H:\\My Drive\\analisa sentimen git\\percobaan 9\\validation_filtered.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Menghapus duplikat\n",
    "data = data.drop_duplicates()\n",
    "\n",
    "# Menampilkan jumlah total data setelah menghapus duplikat\n",
    "print(f\"Jumlah total data setelah menghapus duplikat: {len(data)}\")\n",
    "\n",
    "# Menerapkan preprocessing ke kolom 'content'\n",
    "data['processed_content'] = data['content'].astype(str).apply(preprocess_text)\n",
    "\n",
    "# Menghitung jumlah positif dan negatif\n",
    "jumlah_positif = len(data[data['score'] > 0])\n",
    "jumlah_negatif = len(data[data['score'] < 0])\n",
    "\n",
    "print(f\"Jumlah positif: {jumlah_positif}\")\n",
    "print(f\"Jumlah negatif: {jumlah_negatif}\")\n",
    "\n",
    "# Menyimpan DataFrame yang sudah diproses ke file CSV baru\n",
    "output_file_path = r'H:\\My Drive\\analisa sentimen git\\percobaan 9\\validation_results_processed.csv'\n",
    "data.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Data yang sudah diproses disimpan di: {output_file_path}\")\n",
    "\n",
    "# Menampilkan beberapa contoh hasil preprocessing\n",
    "print(data[['content', 'processed_content']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hisya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\hisya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hisya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File berhasil disimpan di H:\\My Drive\\analisa sentimen git\\percobaan 9\\validation_results_tokenized.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Pastikan Anda sudah mengunduh tokenizer yang diperlukan\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Baca file CSV\n",
    "input_path = r\"H:\\My Drive\\analisa sentimen git\\percobaan 9\\validation_results_processed.csv\"\n",
    "data = pd.read_csv(input_path)\n",
    "\n",
    "# Tokenisasi pada kolom 'content' dan 'processed_content'\n",
    "data['content_tokenized'] = data['content'].apply(lambda x: word_tokenize(str(x)))\n",
    "data['processed_content_tokenized'] = data['processed_content'].apply(lambda x: word_tokenize(str(x)))\n",
    "\n",
    "# Simpan hasilnya ke file CSV baru\n",
    "output_path = r\"H:\\My Drive\\analisa sentimen git\\percobaan 9\\validation_results_tokenized.csv\"\n",
    "data.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"File berhasil disimpan di {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
